%!TEX program = xelatex

\documentclass{comjnl}

\usepackage{polyglossia}
\setdefaultlanguage{english}

\usepackage{amsmath}
\usepackage[backend=biber]{biblatex}
\usepackage[autostyle]{csquotes}
\usepackage{dsfont}
\usepackage{fontenc}
\usepackage{hyperref}
\usepackage{tikz}

\newcommand{\abs}[1]{\lvert #1 \rvert|}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\argmin}{\arg\!\min}

\addbibresource{./Extended_Essay.bib}

\numberwithin{equation}{subsection}

\begin{document}

    \title{Applications and Development of Machine Learning}
    \author{Tarik Onalan}
    \affiliation{Interlake High School}

    \shortauthors{T. Onalan}

    \begin{abstract}

        In the last decade, machine learning has taken centre stage in the computing field. With the
        advent of increasingly powerful computers, giga-, tera-, and even peta-scale computations have
        become possible. In this paper, we will investigate the applications of machine learning in
        these large-scale computations---big data---and how they can affect future development.

    \end{abstract}

    \maketitle

%--------------------------------------------------------------------------------------------------%


    \section{Introduction}
        
        The term ``Machine Learning'' was coined in 1959 by Arthur Samuel, in his paper \textit{Some
        studies in machine learning using the game of checkers}, described as ``a field
        of study that gives computers the ability to learn without being explicitly programmed''. The
        definition still holds, but machine learning, as a science, is now leaps and bounds ahead of
        where it was in 1959. Computers are now capable of analyzing giga- and tera-scale sets of data
        in hours or minutes instead of days, weeks, or even months or years.

        It is essentially impossible to understand big data without machine learning. The computational
        power required to parse and analyze modern data-sets is well beyond the power of all human minds
        ever to be created. With computers, however, we can run calculations faster and with higher
        precision than any human ever could.

        The applications of machine learning are vast, spanning from finance, to health care, to social
        media. Anywhere where there is a large pool of data to be analyzed, machine learning becomes a
        viable option.

%--------------------------------------------------------------------------------------------------%

    \section{Learning Methods}\label{sec:learn}

        \subsection{Supervised Learning}\label{sec:sup}

            Supervised learning is achieved by training a model by optimizing it to fit a set of
            input-output pairs. Given a set of inputs and outputs $\{(x_1,y_1),...,(x_n,y_n)\}$,
            the model that is being trained will seek a function $h : X \to Y$, where $X$ is the
            input and $Y$ is the output. The key fact to remember with this learning method is that
            there is an expected output for every input against which the model's predictions can be
            compared and tuned.

            A good way of visualizing this kind of learning algorithm is with a pond and a fisherman.
            Let us imagine a pond with 3 species of fish: catfish, carp, and goldfish. Each species
            swims in a particular region of the pond. The fisherman does not know which part of the
            pond each species prefers, however.

            The fisherman takes a fish from the pond, and notes its type. They continue to do this,
            noting the fish types. They now know that if they take a fish from (for example) the
            left of the pond, it is most likely going to be a goldfish. They also know that fish
            from the bottom of the pond are catfish, and fish from the right of the pond are most
            likely carp.

            Later, someone asks the fisherman to identify a fish from the pond, except they only
            tell the fisherman what part of the pond the fish was from---the bottom, in this case.
            The fisherman, having learned that fish from the bottom of the pond are most likely
            catfish, replies that the fish is most likely a catfish.

        \subsection{Unsupervised Learning}\label{sec:unsup}

            Unsupervised learning is, as the name suggests, the opposite of supervised learning.
            Instead of constructing a model from input-output pairs, unsupervised learning consists
            of constructing a model solely from an input set. Essentially, given unlabeled data, the
            objective in unsupervised learning is to find any hidden structures that may be present
            in the data, and use those hidden structures to construct a classification or regression
            model. Unsupervised learning does not have to be used alone: it can also be a means to
            an end; after understanding the structure of a dataset, it becomes easier to transition
            to more robust learning methods like supervised learning (\ref{sec:sup}) or
            reinforcement learning (\ref{sec:unsup})

            This algorithm is slightly more difficult to visualize. Let us continue with our
            metaphor with the pond and the fisherman; this time, however, the fisherman, after
            pulling a fish out of the water, is not able to tell exactly what kind of fish it is.
            All they can know is that it came from, say, the left of the pond.

            This time, when someone asks the fisherman to identify a fish from the pond, all the
            fisherman can answer, given their information, is that the fish is of the type that
            comes from, for example, the left of the pond. Essentially, the fisherman grouped the
            fish into different categories: the fish from the left are of one type, the fish from
            the bottom of are another type, and the fish from the right are of yet another type.

        \subsection{Reinforcement Learning}\label{sec:rein}

            Reinforcement learning functions in the following manner: model is presented with an
            input space, and a function to calculate reward; however, in this case, there are no
            explicit input-output pairs, and if the model begins to stray to a sub-optimal path,
            there will be no outright correction of the digression. This method of learning is
            preferred for on-line (real-time) learning, as a constant dataset is not required for
            reinforcement learning. Instead, there must be a balance between exploration of new
            knowledge and exploitation of old knowledge.

            Visualizing this type of learning is slightly different than the previous two. We can
            use the classic multi-armed bandit problem to describe reinforcement learning
            (\cite{auerp}). A bandit must decide which one of $k$ non-identical slot machines to
            play in order to maximize their reward. If the bandit is on a well-paying slot machine,
            the incentive for them to move---exploring new knowledge---is less, while their incentive
            to keep playing the slot machine---exploiting old knowledge---is higher. The opposite is
            true if the bandit is on a poorly-paying slot machine; the bandit is more likely to
            switch machines to try to find a better-paying machine.

%--------------------------------------------------------------------------------------------------%

    \section{Algorithms}\label{sec:alg}

        \subsection{$k$-NN}\label{subsec:knn}

            $k$-nearest neighbors, commonly shortened as $k$-NN, is a popular machine learning
            algorithm because of its relative simplicity to implement. It is non-parametric---
            meaning it does not assume that the input data has any particular structure, making it
            an ideal tool for ``testing'' the structure of a dataset and identifying any basic
            patterns. This property also makes it perform well in unsupervised learning
            (\ref{sec:unsup}).

            When classifying an input $x_n$, for example, it takes the ``majority vote'' of the $k$
            nearest neighbors to the input $x_n$ to determine the class of the input. 

            \begin{equation}\label{eq:basic_knn}
                x_{\theta}=\argmax_{\theta} \abs{k_{\theta} \in \{k^*\}}
            \end{equation}

            \noindent Equation \eqref{eq:basic_knn} demonstrates the ``majority vote'' method.
            The class of an input vector $x$, represented here by $x_{\theta}$ is defined as the
            class---$\theta$---for which the number of nearest neighbors $k_{\theta}$ in in all of
            the possible nearest neighbors $\{k^*\}$ is maximized.

            Variations of the algorithm also vary the ``weight'' of the vote with the inverse of
            the distance. This helps counteract any bias incurred by skewed data; for example, if
            a point were to have three neighbors, one very near and two very far, the ``weighted
            vote'' would ideally have the nearer neighbor have more influence over the class of the
            input vector than the further neighbors.

            \begin{equation}\label{eq:weighted_knn}
                x_{\theta}=\argmin_{\theta} \frac{\sum_n{\text{dist}^m(k_n \in \{k_{\theta}^*\})}}{\abs{k_{\theta} \in \{k^*\}}}
            \end{equation}

            \noindent Equation \eqref{eq:weighted_knn} demonstrates the ``weighted vote'' method.
            The class of an input vector $x$, again represented by $x_{\theta}$, is defined by the
            $\theta$ for which the sums of the distances between the $k$ nearest neighbors of class
            $\theta$ divided by the cardinality of the set $k_{\theta} \in \{k^{*}\}$ is minimized.
            Important to note is that the function $\text{dist}$ is taken to the power of $m$; the
            weighting need not be inverse-linear: it could be inverse-square---arguably the most
            popular---inverse-cubic, inverse-quartic, et cetera.

        \subsection{Support Vector Machines}\label{subsec:svm}

            Support vector machines are supervised learning models (\ref{sec:sup}) that are commonly
            used for classification and regression tasks. With a linear kernel (which is all we
            will go over, for the sake of simplicity), a support vector machine will fit a linear
            model to separate a dataset of two classes (while more than two can be classified, it
            requires extra computation).

            A linear model for a support vector machine is as follows:

            \begin{equation}\label{eq:hyperplane}
                \omega \cdot x_i + b = y_i
            \end{equation}

            \noindent where $\omega$ is the matrix of weights, $x_i$ is the input vector, $b$ is the
            bias, and $y_i$ is the class. The support vector machine finds the parameters $\omega$ and $b$ for the
            hyperplane that separate the input dataset into two classes: $\{-1,1\}$. This is accomplished by
            maximizing the distance to elements of either class on both sides of the hyperplane. Maximizing
            the distance is accomplished with the aid of two solutions to the hyperplane defined in
            equation \eqref{eq:hyperplane}:

            \begin{equation}\label{eq:sepclass}
                \begin{cases}
                    \omega \cdot x_i + b \geq 1 & \quad \text{Class above line} \\
                    \omega \cdot x_i + b \leq -1 & \quad \text{Class below line}
                \end{cases}
            \end{equation}

            \noindent As $y_i$ is always either $-1$ or $1$, we can simplify this further:

            \begin{equation}\label{eq:bothclass}
                y_i(\omega \cdot x_i + b) \geq 1 \quad \forall i \in [1,n]
            \end{equation}

            \noindent Finally we can take the equations in \eqref{eq:sepclass} to derive our
            objective function:

            \begin{equation*}
                \begin{cases}
                    \omega \cdot x_i + b = 1 \\
                    \omega \cdot x_j + b = -1
                \end{cases}
            \end{equation*}

            \noindent What we are assuming here is that $x_i$ represents the element of class $1$
            nearest to our line, and that $x_j$ represents the element of class $-1$ nearest to our
            line.

            \begin{equation*}
                \omega \cdot (x_i-x_j) = 2
            \end{equation*}

            \begin{equation}\label{eq:protoobj}
                (x_i-x_j)=\frac{2}{\norm{\omega}}
            \end{equation}

            \noindent Now, if we remember, we were trying to maximize the difference between the
            elements of the respective classes $\{-1,1\}$ that were closest to our line; we can use
            equation \eqref{eq:protoobj} to define our objective function with respect to $\omega$:

            \begin{equation*}
                \argmax_{(i,j)} x_i-x_j
            \end{equation*}

            \begin{equation}\label{eq:obj}
                \argmin_{\omega} \frac{2}{\norm{\omega}}
            \end{equation}

            \noindent With the objective function, we can now solve for $\omega$ by putting equation
            \eqref{eq:obj} under the constraint of equation \eqref{eq:bothclass}, and solving for
            the weights.

        \subsection{Neural Network}\label{subsec:nnet}

            Neural networks are arguably the most publicized of the machine learning algorithms,
            likely due to their functionality. In essence, they function the same way a small brain
            would, weighing inputs and providing an output based on a threshold function. Each
            perceptron---the neural network's equivalent of a neuron---weighs one metric, and
            passes on the value to the next perceptron.
            \\[0.5cm]
            \begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=2.5cm]
                \tikzstyle{every pin edge}=[<-,shorten <=1pt]
                \tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
                \tikzstyle{input neuron}=[neuron, fill=green!50];
                \tikzstyle{output neuron}=[neuron, fill=red!50];
                \tikzstyle{hidden neuron}=[neuron, fill=blue!50];
                \tikzstyle{annot} = [text width=4em, text centered]

                \foreach \name / \y in {1,...,4}
                    \node[input neuron, pin=left:$x_{\y}$] (I-\name) at (0,-\y) {};

                \foreach \name / \y in {1,...,5}
                    \path[yshift=0.5cm]
                        node[hidden neuron] (H-\name) at (2.5cm,-\y cm) {};

                \node[output neuron,pin={[pin edge={->}]right:Output}, right of=H-3] (O) {};

                \foreach \source in {1,...,4}
                    \foreach \dest in {1,...,5}
                        \path (I-\source) edge (H-\dest);

                \foreach \source in {1,...,5}
                    \path (H-\source) edge (O);

                \node[annot,above of=H-1, node distance=1cm] (hl) {Hidden layer};
                \node[annot,left of=hl] {Input layer};
                \node[annot,right of=hl] {Output layer};
            \end{tikzpicture}
            \\[0.5cm]
            Neural networks are rather popular in practical use as well, mostly because they
            are able to relatively accurately map unknown functions of multiple parameters.
            Additionally, they are capable of learning with all three learning methods shown in
            section (\ref{sec:learn}). This makes them a versatile tool in any data scientist's
            toolbox.

            Let us do the derivation for a simple single hidden layer perceptron network with
            backpropagation. The first step is to determine an activation function; we will choose
            the sigmoid function:

            \begin{equation}\label{eq:act}
                \varphi(x)=\frac{1}{1+e^x}
            \end{equation}

            \noindent Now that we have an activation function, we can define the system of equations
            describing a neural network with one hidden layer:

            \begin{equation}\label{eq:basicnnet}
                \begin{cases}
                    S_i(x)=\sum_n \varphi(\omega_{ni}x_n) & \quad \text{Input layer} \\
                    S_j(x)=\sum_n \varphi(\omega_{nj}S_i(x)) & \quad \text{Hidden layer} \\
                    S_k(x)=\omega_kS_j(x) & \quad \text{Output Layer (bias)}
                \end{cases}
            \end{equation}

            \noindent From there, we can describe our error function:

            \begin{equation}\label{eq:error}
                E(x_i^*) = \sum_{x_i \in x_i^*} (S_k(x_i)-y_i)^2
            \end{equation}

            \noindent The error function described in equation \eqref{eq:error} can then be used to
            update the individual weights

            \begin{equation}\label{eq:update}
                \begin{cases}
                    \omega_i = \omega_i - \alpha\frac{\partial E}{\partial \omega_i} \\
                    \omega_j = \omega_j - \alpha\frac{\partial E}{\partial \omega_j} \\
                    \omega_k = \omega_k - \alpha\frac{\partial E}{\partial \omega_k}
                \end{cases}
            \end{equation}

            \noindent where $\alpha \in (0,1]$, usually between $0.02$ and $0.05$, is used to
            control the speed of the gradient descent.

%--------------------------------------------------------------------------------------------------%

    \section{Finance}

        Machine learning has become heavily relied upon in the field of finance, partially due to
        the fact that the regression of market data is one of many variables---other stock prices,
        financial outlooks, financial news---and is very ``noisy'', with many random variations in
        a stock price in a given day. As such, neural networks---are heavily used in the field of
        finance, as they are exceedingly good at mapping unknown functions of multiple variables
        (\ref{subsec:nnet}). A neural network could predict the rise and fall of certain stocks
        given a large feature vector containing, say, the profit margin of a company, the national
        inflation rate, the number of jobs created the previous quarter, et cetera. At the right
        $\alpha$ values (\ref{subsec:nnet}), a neural network could also be very resistant to
        ``overfitting'': describing statistical ``noise'' more than the actual data.

        Especially now, with the production of increasingly powerful supercomputers, more and more
        people are using machine learning in financial applications.  Supercomputers can be used to
        not only calculate the trends of stocks, but calculate them in real time, learning on-line.
        Advanced algorithms take into account not only current stocks, but global market outlook,
        important corporate or governmental incidents, and similar qualitative data in addition to
        the raw quantitative data to augment their estimates of possible future trends. This
        proves itself to be a more robust method of regression, because on its own, financial
        data is quite difficult to regress; a dataset of stock prices from six months ago to now
        would not guarantee a valid model of the future. However, with more qualitative data, it
        becomes possible to predict market prices with slightly higher accuracy.

        Important to note, of course, is that the algorithms used in financial applications are
        often specifically tailored for those applications. Herbert Simon, a computer scientist,
        economist, and psychologist, in his 1976 paper introduced a more human-like, symbol-based
        approach to modeling financial markets \cite{simonh}. This approach was very much literal;
        much like what a ``symbol'' is to a human, the computer would analyze a situation, compare
        it to previously encountered situations, and predict a result based on what the previous
        examples were. However, there were inherent difficulties in enacting the approach: where did
        the ``symbols'' come from? How do they adapt to new situations? How are they influenced by
        their environment? In recent history, there seems to have been a partial resolution to these
        questions in the form of autonomous agents: not quite symbol-based, but making its own
        decisions based on past data, buying and selling stocks \cite{chens, calvacanter}.

%--------------------------------------------------------------------------------------------------%

    \section{Health Care}

        Machine learning in the health care sector is centred more around clustering; support vector
        machines (\ref{subsec:svm}), decision trees, k-nearest neighbor (\ref{subsec:knn}), and
        k-means clustering are some of the more popular algorithms in use in the computational
        biology fields, specifically. With the advent of genome research, scientists have to search
        for genetic anomalies that could be signs of future disease. Clustering algorithms like k-NN
        (\ref{subsec:knn}) are useful in this application because the computational power required
        is not as demanding as an unsupervised neural network, while they are still able to identify
        any important structures in the data. These unsupervised clustering algorithms are generally
        paired with an overarching supervised algorithm; after identifying clusters of possible
        genes, the supervised algorithm takes over and begins mapping the clusters to symptoms
        or diseases.

        Genomes themselves pose an interesting computational challenge; a single genome is around
        1.5 gigabytes, which means that loading and processing a genome is very memory and
        computation intensive. After loading a genome, the next objective is identifying outliers.
        However, every human can have different mutations on different genes, some of which may not
        have any significant health-related effects. Genomes must be cross-checked with wide
        varieties of populations to train an algorithm to ignore certain genes and be suspicious of
        others.

        Let us also note that machine learning can be used in other regions of health care as well.
        Just as the ability to effectively generalize data and ignore ``noise'' makes machine
        learning effective in financial applications, it also makes machine learning effective in
        health care. EEG, ECG, heart rate, blood pressure, and similar measurements can all be
        processed at once using machine learning, while reducing the likelihood that noise would
        significantly affect the prognosis.

        Arguably the most important part of machine learning in health care, however, is that it
        helps save human lives: take, for example lymphoma. Diffuse large B-Cell lymphoma, the most
        common lymphoid malignancy in adults, is curable in less than 50\% of cases. However, with
        machine learning, doctors are able to identify the type of chemotherapy drug a tumor is most
        susceptible to, possibly increasing the survival rate of the patient (\cite{shippm}).        

%--------------------------------------------------------------------------------------------------%

    \section{Social Media}

        Machine learning in social media is a trend that has developed fairly recently. The common
        algorithms in use in social media---for the purpose of news displays, post displays, et
        cetera---are quite varied, ranging from classification, to clustering, to regression.
        This is because the possible applications of machine learning in social media are varied
        and numerous: advertising, recommendations, profiling, predictions, et cetera. 

        Advertising and recommendations are (often to our chagrin) becoming increasingly common on
        the internet. They have become a lucrative source of revenue for many companies, some of
        which have access to millions of gigabytes of data. User data in particular is what is
        important; every advertisement is tailored towards a certain user. Machine learning
        algorithms analyze everything from a user's search history, website interactions, comments,
        posts, et cetera, to decide which advertisement or recommendation to place in the user's
        browser window (\cite{perlichc}).

        Machine learning can also be used to predict characteristics about a user: male or female,
        liberal or conservative, introvert or extrovert (\cite{pennacchiottim}). While this does
        inevitably tie back to advertising and recommendations, it presents an slightly different
        problem. Sentences need to be encoded into a form machine learning models can parse; natural
        language processing is required to analyze posts and comments for biases; analysis of known
        users is required for a ``baseline''. Essentially, while user information is used in
        choosing what advertisement goes where, extracting that information is another problem
        entirely.

        In addition, the rise of what is now known as the ``personal assistant'' on many electronic
        devices requires the prediction of user actions to streamline user experience. By training
        a model on a user's previous actions, that model can then be used to predict that user's
        future actions (\cite{stuartl}). It is important to note that we believe the prediction of
        user actions still falls within the realm of social media, as it is directly dealing with
        the end user and their actions.

%--------------------------------------------------------------------------------------------------%

    \section{Conclusion}\label{sec:concl}

        Machine learning is a field with a list of applications that is constantly growing in our
        age of big data and instant information. Every day, 2.5 exabytes (2.5 billion gigabytes) of
        data are created on the internet, more than any human---or computer, for that matter---can
        handle in our day and age.

        Simply speaking, however, humans are no longer---and never quite were---a viable option for
        analyzing large, multidimensional datasets; they are simply too slow and too inaccurate. A
        computer paired with the proper algorithm, however, can be accurate and performant. It is
        fascinating to think what could be extrapolated from just one percent of the internet's
        daily data production.

        Machine learning will inevitably continue to grow in prominence in our daily lives, becoming
        an increasingly integral part of our society as the amount of data we generate grows. This,
        essentially, brings us back to our main point: it is impossible to understand big data
        without machine learning.

%--------------------------------------------------------------------------------------------------%

    \nocite{*}
    \printbibliography

\end{document}
