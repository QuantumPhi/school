@article{samuela,
    author = {Arthur L. Samuel},
    title = {Some studies in machine learning using the game of Checkers},
    journal = {IBM JOURNAL OF RESEARCH AND DEVELOPMENT},
    year = {1959},
    pages = {71-105}
}

@article{moguerzaj,
    jstor_articletype = {research-article},
    title = {Support Vector Machines with Applications},
    author = {Moguerza, Javier M. and Muñoz, Alberto},
    journal = {Statistical Science},
    jstor_issuetitle = {},
    volume = {21},
    number = {3},
    jstor_formatteddate = {Aug., 2006},
    pages = {322-336},
    url = {http://www.jstor.org/stable/27645765},
    ISSN = {08834237},
    abstract = {Support vector machines (SVMs) appeared in the early nineties as optimal margin classifiers in the context of Vapnik's statistical learning theory. Since then SVMs have been successfully applied to real-world data analysis problems, often providing improved results compared with other techniques. The SVMs operate within the framework of regularization theory by minimizing an empirical risk in a well-posed and consistent way. A clear advantage of the support vector approach is that sparse solutions to classification and regression problems are usually obtained: only a few samples are involved in the determination of the classification or regression functions. This fact facilitates the application of SVMs to problems that involve a large amount of data, such as text processing and bioinformatics tasks. This paper is intended as an introduction to SVMs and their applications, emphasizing their key features. In addition, some algorithmic extensions and illustrative real-world applications of SVMs are shown.},
    language = {English},
    year = {2006},
    publisher = {Institute of Mathematical Statistics},
    copyright = {Copyright © 2006 Institute of Mathematical Statistics}
}

@article{devroyel,
    jstor_articletype = {research-article},
    title = {On the Strong Universal Consistency of Nearest Neighbor Regression Function Estimates},
    author = {Devroye, Luc and Gyorfi, Laszlo and Krzyzak, Adam and Lugosi, Gabor},
    journal = {The Annals of Statistics},
    jstor_issuetitle = {},
    volume = {22},
    number = {3},
    jstor_formatteddate = {Sep., 1994},
    pages = {1371-1385},
    url = {http://www.jstor.org/stable/2242230},
    ISSN = {00905364},
    abstract = {Two results are presented concerning the consistency of the k-nearest neighbor regression estimate. We show that all modes of convergence in L1 (in probability, almost sure, complete) are equivalent if the regression variable is bounded. Under the additional conditional k/log n → ∞ we also obtain the strong universal consistency of the estimate.},
    language = {English},
    year = {1994},
    publisher = {Institute of Mathematical Statistics},
    copyright = {Copyright © 1994 Institute of Mathematical Statistics}
}

@article{hofmannt,
    jstor_articletype = {research-article},
    title = {Kernel Methods in Machine Learning},
    author = {Hofmann, Thomas and Schölkopf, Bernhard and Smola, Alexander J.},
    journal = {The Annals of Statistics},
    jstor_issuetitle = {},
    volume = {36},
    number = {3},
    jstor_formatteddate = {Jun., 2008},
    pages = {1171-1220},
    url = {http://www.jstor.org/stable/25464664},
    ISSN = {00905364},
    abstract = {We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data. We cover a wide range of methods, ranging from binary classifiers to sophisticated methods for estimation with structured data.},
    language = {English},
    year = {2008},
    publisher = {Institute of Mathematical Statistics},
    copyright = {Copyright © 2008 Institute of Mathematical Statistics}
}

@article{warnerb,
    jstor_articletype = {research-article},
    title = {Understanding Neural Networks as Statistical Tools},
    author = {Warner, Brad and Misra, Manavendra},
    journal = {The American Statistician},
    jstor_issuetitle = {},
    volume = {50},
    number = {4},
    jstor_formatteddate = {Nov., 1996},
    pages = {284-293},
    url = {http://www.jstor.org/stable/2684922},
    ISSN = {00031305},
    abstract = {Neural networks have received a great deal of attention over the last few years. They are being used in the areas of prediction and classification, areas where regression models and other related statistical techniques have traditionally been used. In this paper we discuss neural networks and compare them to regression models. We start by exploring the history of neural networks. This includes a review of relevant literature on the topic of neural networks. Neural network nomenclature is then introduced, and the backpropagation algorithm, the most widely used learning algorithm, is derived and explained in detail. A comparison between regression analysis and neural networks in terms of notation and implementation is conducted to aid the reader in understanding neural networks. We compare the performance of regression analysis with that of neural networks on two simulated examples and one example on a large dataset. We show that neural networks act as a type of nonparametric regression model, enabling us to model complex functional forms. We discuss when it is advantageous to use this type of model in place of a parametric regression model, as well as some of the difficulties in implementation.},
    language = {English},
    year = {1996},
    publisher = {Taylor & Francis, Ltd. on behalf of the American Statistical Association},
    copyright = {Copyright © 1996 American Statistical Association}
}

@article{grangerc,
    title = {Forecasting stock market prices: Lessons for forecasters},
    journal = {International Journal of Forecasting},
    volume = {8},
    number = {1},
    pages = {3 - 13},
    year = {1992},
    issn = {0169-2070},
    doi = {http://dx.doi.org/10.1016/0169-2070(92)90003-R},
    url = {http://www.sciencedirect.com/science/article/pii/016920709290003R},
    author = {Clive W.J. Granger},
    abstract = {In recent years a variety of models which apparently forecast changes in stock market prices have been introduced. Some of these are summarised and interpreted. Nonlinear models are particularly discussed, with a switching regime, from forecastable to non-forecastable, the switch depending on volatility levels, relative earnings/price ratios, size of company, and calendar effects. There appear to be benefits from disaggregation and for searching for new causal variables. The possible lessons for forecasters are emphasised and the relevance for the Efficient Market Hypothesis is discussed.}
}

@article{chens,
    title = {Computational intelligence in economics and finance: Carrying on the legacy of Herbert Simon},
    journal = {Information Sciences},
    volume = {170},
    number = {1},
    pages = {121 - 131},
    year = {2005},
    note = {Computational Intelligence in Economics and Finance},
    issn = {0020-0255},
    doi = {http://dx.doi.org/10.1016/j.ins.2003.11.006},
    url = {http://www.sciencedirect.com/science/article/pii/S0020025503004444},
    author = {Shu-Heng Chen},
    abstract = {This is an editorial guide for the special issue on computational intelligence (CI) in economics and finance. A historical introduction to the background is given. This research paradigm is traced back to Herbert Simon, who, as a founder of artificial intelligence, pioneered the applications of \{AI\} to economics. The move from the classical \{AI\} to \{CI\} indicates a continuation of the legacy of Herbert Simon. Computational intelligence has proved to be a constructive foundation for economics. In responding to what Herbert Simon referred as procedural rationality, our study of bounded rationality has been enriched by bringing autonomous agents into the economic analysis.}
}

@article{simonh,
    author = {Newell, Allen and Simon, Herbert A.},
    title = {Computer Science As Empirical Inquiry: Symbols and Search},
    journal = {Commun. ACM},
    issue_date = {March 1976},
    volume = {19},
    number = {3},
    month = mar,
    year = {1976},
    issn = {0001-0782},
    pages = {113--126},
    numpages = {14},
    url = {http://doi.acm.org/10.1145/360018.360022},
    doi = {10.1145/360018.360022},
    acmid = {360022},
    publisher = {ACM},
    address = {New York, NY, USA}
}

@article{sajdap,
    author = {Sajda, Paul},
    title = {MACHINE LEARNING FOR DETECTION AND DIAGNOSIS OF DISEASE},
    journal = {Annual Review of Biomedical Engineering},
    volume = {8},
    number = {1},
    pages = {537-565},
    year = {2006},
    doi = {10.1146/annurev.bioeng.8.061505.095802},
    note ={PMID: 16834566},
    url = {http://dx.doi.org/10.1146/annurev.bioeng.8.061505.095802},
    eprint = {http://dx.doi.org/10.1146/annurev.bioeng.8.061505.095802},
    abstract = { AbstractMachine learning offers a principled approach for developing sophisticated, automatic, and objective algorithms for analysis of high-dimensional and multimodal biomedical data. This review focuses on several advances in the state of the art that have shown promise in improving detection, diagnosis, and therapeutic monitoring of disease. Key in the advancement has been the development of a more in-depth understanding and theoretical analysis of critical issues related to algorithmic construction and learning theory. These include trade-offs for maximizing generalization performance, use of physically realistic constraints, and incorporation of prior knowledge and uncertainty. The review describes recent developments in machine learning, focusing on supervised and unsupervised linear methods and Bayesian inference, which have made significant impacts in the detection and diagnosis of disease in biomedicine. We describe the different methodologies and, for each, provide examples of their application to specific domains in biomedical diagnostics.}
}

@article{shippm,
    author={Shipp, Margaret A. and Ross, Ken N. and Tamayo, Pablo and Weng, Andrew P. and Kutok, Jeffery L. and Aguiar, Ricardo C.T. and Gaasenbeek, Michelle and Angelo, Michael and Reich, Michael and Pinkus, Geraldine S. and Ray, Tane S. and Koval, Margaret A. and Last, Kim W. and Norton, Andrew and Lister, T. Andrew and Mesirov, Jill and Neuberg, Donna S. and Lander, Eric S. and Aster, Jon C. and Golub, Todd R.},
    title={Diffuse large B-cell lymphoma outcome prediction by gene-expression profiling and supervised machine learning},
    journal={Nat Med},
    year={2002},
    month={Jan},
    volume={8},
    number={1},
    pages={68-74},
    issn={1078-8956},
    doi={10.1038/nm0102-68},
    url={http://dx.doi.org/10.1038/nm0102-68}
}

@article{Kukar199925,
    title = {Analysing and improving the diagnosis of ischaemic heart disease with machine learning},
    journal = {Artificial Intelligence in Medicine},
    volume = {16},
    number = {1},
    pages = {25 - 50},
    year = {1999},
    note = {Data Mining Techniques and Applications in Medicine},
    issn = {0933-3657},
    doi = {http://dx.doi.org/10.1016/S0933-3657(98)00063-3},
    url = {http://www.sciencedirect.com/science/article/pii/S0933365798000633},
    author = {Matjaž Kukar and Igor Kononenko and Ciril Grošelj and Katarina Kralj and Jure Fettich},
    abstract = {Ischaemic heart disease is one of the world’s most important causes of mortality, so improvements and rationalization of diagnostic procedures would be very useful. The four diagnostic levels consist of evaluation of signs and symptoms of the disease and \{ECG\} (electrocardiogram) at rest, sequential \{ECG\} testing during the controlled exercise, myocardial scintigraphy, and finally coronary angiography (which is considered to be the reference method). Machine learning methods may enable objective interpretation of all available results for the same patient and in this way may increase the diagnostic accuracy of each step. We conducted many experiments with various learning algorithms and achieved the performance level comparable to that of clinicians. We also extended the algorithms to deal with non-uniform misclassification costs in order to perform \{ROC\} analysis and control the trade-off between sensitivity and specificity. The \{ROC\} analysis shows significant improvements of sensitivity and specificity compared to the performance of the clinicians. We further compare the predictive power of standard tests with that of machine learning techniques and show that it can be significantly improved in this way.}
}

@article{perlichc,
    year={2014},
    issn={0885-6125},
    journal={Machine Learning},
    volume={95},
    number={1},
    doi={10.1007/s10994-013-5375-2},
    title={Machine learning for targeted display advertising: transfer learning in action},
    url={http://dx.doi.org/10.1007/s10994-013-5375-2},
    publisher={Springer US},
    keywords={Transfer learning; Display advertising; Predictive modeling},
    author={Perlich, C. and Dalessandro, B. and Raeder, T. and Stitelman, O. and Provost, F.},
    pages={103-127},
    language={English}
}

@article{stuartl,
    title = {User Modeling via Machine Learning and Rule-Based Reasoning to Understand and Predict Errors in Survey Systems},
    author = {Stuart, Leonard Cleve},
    year = {2013},
    url = {http://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1086}
}

@inproceedings{calvacanter,
    author={Cavalcante, R.C. and Oliveira, A.L.I.},
    booktitle={Neural Networks (IJCNN), 2014 International Joint Conference on},
    title={An autonomous trader agent for the stock market based on online sequential extreme learning machine ensemble},
    year={2014},
    pages={1424-1431},
    abstract={Financial markets are very important to the economical and social organization of modern society. In this kind of market, the success of an investor depends on the quality of the information he uses to trade in the market, and on how fast he is able to take decisions. In the literature, several statistical and soft computing mechanisms have been proposed in order to support investors decision in the financial market. In this work we propose an autonomous trader agent that is able to compute technical indicators of the stock market and take decisions on buying or selling stocks. Our trader agent is based on a single hidden layer feedforward (SLFN) ensemble trained with online sequential extreme learning machine (OS-ELM), a variant of ELM that is able to learn data one-by-one and dynamically accommodate changes in the market. In addition, we propose a set of trading rules that guides the trader agent in order to improve the potential profit. Experimental results on real dataset from Brazilian stock market showed that our proposed trader agent based on OS-ELM ensemble is able to increase the financial gain when compared with other approaches proposed in literature.},
    keywords={decision support systems;economic indicators;feedforward neural nets;financial data processing;investment;learning (artificial intelligence);profitability;software agents;Brazilian stock market;OS-ELM;SLFN ensemble training;autonomous trader agent;dynamic market change accommodation;economical organization;financial gain;financial markets;information quality;investor decision support;investor success;online sequential extreme learning machine ensemble;potential profit improvement;single hidden layer feedforward ensemble;social organization;stock buying;stock market trading;stock selling;technical indicator computation;trading rules;Backpropagation;Feedforward neural networks;Forecasting;Stock markets;Time series analysis;Training},
    doi={10.1109/IJCNN.2014.6889870},
    month={July}
}

@inproceedings{pennacchiottim,
    abstract = {This paper addresses the task of user classification in social media, with an application to Twitter. We automatically infer the values of user attributes such as political orientation or ethnicity by leveraging observable information such as the user behavior, network structure and the linguistic content of the user\&rsquo;s Twitter feed. We employ a machine learning approach which relies on a comprehensive set of features derived from such user information. We report encouraging experimental results on 3 tasks with different characteristics: political affiliation detection, ethnicity identification and detecting affinity for a particular business. Finally, our analysis shows that rich linguistic features prove consistently valuable across the 3 tasks and show great promise for additional user classification needs.},
    author = {Pennacchiotti, Marco and Popescu, Ana-Maria},
    keywords = {cosn2014, done, icwsm, metric-brief-0, metric-consent-0, metric-irb-0, metric-length-1, metric-n-1, metric-processing-0, metric-protocol-0, metric-sampling-1, metric-shared-0, metric-sourcesns-1, yes},
    priority = {2},
    title = {{A Machine Learning Approach to Twitter User Classification}},
    url = {http://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/view/2886},
    year = {2011}
}

@book{auerp,
    author={Auer, P. and Cesa-Bianchi, N. and Freund, Y. and Schapire, Robert E.},
    booktitle={Foundations of Computer Science, 1995. Proceedings., 36th Annual Symposium on},
    title={Gambling in a rigged casino: The adversarial multi-armed bandit problem},
    year={1995},
    pages={322-331},
    abstract={In the multi-armed bandit problem, a gambler must decide which arm of K non-identical slot machines to play in a sequence of trials so as to maximize his reward. This classical problem has received much attention because of the simple model it provides of the trade-off between exploration (trying out each arm to find the best one) and exploitation (playing the arm believed to give the best payoff). Past solutions for the bandit problem have almost always relied on assumptions about the statistics of the slot machines. In this work, we make no statistical assumptions whatsoever about the nature of the process generating the payoffs of the slot machines. We give a solution to the bandit problem in which an adversary, rather than a well-behaved stochastic process, has complete control over the payoffs. In a sequence of T plays, we prove that the expected per-round payoff of our algorithm approaches that of the best arm at the rate O(T-1/3), and we give an improved rate of convergence when the best arm has fairly low payoff. We also consider a setting in which the player has a team of “experts” advising him on which arm to play; here, we give a strategy that will guarantee expected payoff close to that of the best expert. Finally, we apply our result to the problem of learning to play an unknown repeated matrix game against an all-powerful adversary},
    keywords={game theory;stochastic games;bandit problem;matrix game;multi-armed bandit problem;rate of convergence;slot machines;well-behaved stochastic process;Arm;Communication networks;Convergence;Costs;Machine learning;Process control;Routing;Statistics;Stochastic processes},
    doi={10.1109/SFCS.1995.492488},
    ISSN={0272-5428},
    month={Oct}
}