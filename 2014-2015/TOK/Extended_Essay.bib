@article{samuela,
    author = {Arthur L. Samuel},
    title = {Some studies in machine learning using the game of Checkers},
    journal = {IBM JOURNAL OF RESEARCH AND DEVELOPMENT},
    year = {1959},
    pages = {71-105}
}

@article{moguerzaj,
    jstor_articletype = {research-article},
    title = {Support Vector Machines with Applications},
    author = {Moguerza, Javier M. and Muñoz, Alberto},
    journal = {Statistical Science},
    jstor_issuetitle = {},
    volume = {21},
    number = {3},
    jstor_formatteddate = {Aug., 2006},
    pages = {pp. 322-336},
    url = {http://www.jstor.org/stable/27645765},
    ISSN = {08834237},
    abstract = {Support vector machines (SVMs) appeared in the early nineties as optimal margin classifiers in the context of Vapnik's statistical learning theory. Since then SVMs have been successfully applied to real-world data analysis problems, often providing improved results compared with other techniques. The SVMs operate within the framework of regularization theory by minimizing an empirical risk in a well-posed and consistent way. A clear advantage of the support vector approach is that sparse solutions to classification and regression problems are usually obtained: only a few samples are involved in the determination of the classification or regression functions. This fact facilitates the application of SVMs to problems that involve a large amount of data, such as text processing and bioinformatics tasks. This paper is intended as an introduction to SVMs and their applications, emphasizing their key features. In addition, some algorithmic extensions and illustrative real-world applications of SVMs are shown.},
    language = {English},
    year = {2006},
    publisher = {Institute of Mathematical Statistics},
    copyright = {Copyright © 2006 Institute of Mathematical Statistics}
}

@article{devroyel,
    jstor_articletype = {research-article},
    title = {On the Strong Universal Consistency of Nearest Neighbor Regression Function Estimates},
    author = {Devroye, Luc and Gyorfi, Laszlo and Krzyzak, Adam and Lugosi, Gabor},
    journal = {The Annals of Statistics},
    jstor_issuetitle = {},
    volume = {22},
    number = {3},
    jstor_formatteddate = {Sep., 1994},
    pages = {pp. 1371-1385},
    url = {http://www.jstor.org/stable/2242230},
    ISSN = {00905364},
    abstract = {Two results are presented concerning the consistency of the k-nearest neighbor regression estimate. We show that all modes of convergence in L1 (in probability, almost sure, complete) are equivalent if the regression variable is bounded. Under the additional conditional k/log n → ∞ we also obtain the strong universal consistency of the estimate.},
    language = {English},
    year = {1994},
    publisher = {Institute of Mathematical Statistics},
    copyright = {Copyright © 1994 Institute of Mathematical Statistics}
}

@article{hofmannt,
    jstor_articletype = {research-article},
    title = {Kernel Methods in Machine Learning},
    author = {Hofmann, Thomas and Schölkopf, Bernhard and Smola, Alexander J.},
    journal = {The Annals of Statistics},
    jstor_issuetitle = {},
    volume = {36},
    number = {3},
    jstor_formatteddate = {Jun., 2008},
    pages = {pp. 1171-1220},
    url = {http://www.jstor.org/stable/25464664},
    ISSN = {00905364},
    abstract = {We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data. We cover a wide range of methods, ranging from binary classifiers to sophisticated methods for estimation with structured data.},
    language = {English},
    year = {2008},
    publisher = {Institute of Mathematical Statistics},
    copyright = {Copyright © 2008 Institute of Mathematical Statistics}
}

@article{warnerb,
    jstor_articletype = {research-article},
    title = {Understanding Neural Networks as Statistical Tools},
    author = {Warner, Brad and Misra, Manavendra},
    journal = {The American Statistician},
    jstor_issuetitle = {},
    volume = {50},
    number = {4},
    jstor_formatteddate = {Nov., 1996},
    pages = {pp. 284-293},
    url = {http://www.jstor.org/stable/2684922},
    ISSN = {00031305},
    abstract = {Neural networks have received a great deal of attention over the last few years. They are being used in the areas of prediction and classification, areas where regression models and other related statistical techniques have traditionally been used. In this paper we discuss neural networks and compare them to regression models. We start by exploring the history of neural networks. This includes a review of relevant literature on the topic of neural networks. Neural network nomenclature is then introduced, and the backpropagation algorithm, the most widely used learning algorithm, is derived and explained in detail. A comparison between regression analysis and neural networks in terms of notation and implementation is conducted to aid the reader in understanding neural networks. We compare the performance of regression analysis with that of neural networks on two simulated examples and one example on a large dataset. We show that neural networks act as a type of nonparametric regression model, enabling us to model complex functional forms. We discuss when it is advantageous to use this type of model in place of a parametric regression model, as well as some of the difficulties in implementation.},
    language = {English},
    year = {1996},
    publisher = {Taylor & Francis, Ltd. on behalf of the American Statistical Association},
    copyright = {Copyright © 1996 American Statistical Association}
}